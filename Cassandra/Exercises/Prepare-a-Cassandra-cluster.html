<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_yjwouxw703ue-0{list-style-type:none}ul.lst-kix_yjwouxw703ue-1{list-style-type:none}.lst-kix_pmrqfhveh3bt-5>li:before{content:"\0025a0  "}.lst-kix_pmrqfhveh3bt-4>li:before{content:"\0025cb  "}.lst-kix_d7thuibe8wvo-1>li:before{content:"\0025cb  "}.lst-kix_pmrqfhveh3bt-3>li:before{content:"\0025cf  "}.lst-kix_d7thuibe8wvo-0>li:before{content:"\0025cf  "}.lst-kix_d7thuibe8wvo-2>li:before{content:"\0025a0  "}.lst-kix_d7thuibe8wvo-3>li:before{content:"\0025cf  "}.lst-kix_pmrqfhveh3bt-2>li:before{content:"\0025a0  "}ul.lst-kix_yjwouxw703ue-2{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-5{list-style-type:none}ul.lst-kix_yjwouxw703ue-3{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-6{list-style-type:none}ul.lst-kix_yjwouxw703ue-4{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-3{list-style-type:none}ul.lst-kix_yjwouxw703ue-5{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-4{list-style-type:none}ul.lst-kix_yjwouxw703ue-6{list-style-type:none}.lst-kix_pmrqfhveh3bt-1>li:before{content:"\0025cb  "}ul.lst-kix_pmrqfhveh3bt-1{list-style-type:none}ul.lst-kix_yjwouxw703ue-7{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-2{list-style-type:none}ul.lst-kix_yjwouxw703ue-8{list-style-type:none}.lst-kix_pmrqfhveh3bt-0>li:before{content:"\0025cf  "}ul.lst-kix_pmrqfhveh3bt-0{list-style-type:none}.lst-kix_d7thuibe8wvo-8>li:before{content:"\0025a0  "}.lst-kix_d7thuibe8wvo-7>li:before{content:"\0025cb  "}ul.lst-kix_pmrqfhveh3bt-7{list-style-type:none}ul.lst-kix_pmrqfhveh3bt-8{list-style-type:none}.lst-kix_d7thuibe8wvo-5>li:before{content:"\0025a0  "}.lst-kix_d7thuibe8wvo-4>li:before{content:"\0025cb  "}.lst-kix_d7thuibe8wvo-6>li:before{content:"\0025cf  "}.lst-kix_hvza1ngnjmlh-3>li:before{content:"\0025cf  "}.lst-kix_hvza1ngnjmlh-4>li:before{content:"\0025cb  "}.lst-kix_hvza1ngnjmlh-0>li:before{content:"\0025cf  "}.lst-kix_hvza1ngnjmlh-8>li:before{content:"\0025a0  "}.lst-kix_hvza1ngnjmlh-1>li:before{content:"\0025cb  "}.lst-kix_hvza1ngnjmlh-2>li:before{content:"\0025a0  "}ul.lst-kix_d7thuibe8wvo-8{list-style-type:none}ul.lst-kix_d7thuibe8wvo-7{list-style-type:none}ul.lst-kix_d7thuibe8wvo-6{list-style-type:none}ul.lst-kix_d7thuibe8wvo-5{list-style-type:none}ul.lst-kix_d7thuibe8wvo-4{list-style-type:none}ul.lst-kix_d7thuibe8wvo-3{list-style-type:none}ul.lst-kix_d7thuibe8wvo-2{list-style-type:none}.lst-kix_hvza1ngnjmlh-7>li:before{content:"\0025cb  "}ul.lst-kix_d7thuibe8wvo-1{list-style-type:none}ul.lst-kix_d7thuibe8wvo-0{list-style-type:none}.lst-kix_hvza1ngnjmlh-5>li:before{content:"\0025a0  "}.lst-kix_hvza1ngnjmlh-6>li:before{content:"\0025cf  "}.lst-kix_yjwouxw703ue-4>li:before{content:"\0025cb  "}.lst-kix_yjwouxw703ue-6>li:before{content:"\0025cf  "}.lst-kix_pmrqfhveh3bt-6>li:before{content:"\0025cf  "}.lst-kix_yjwouxw703ue-5>li:before{content:"\0025a0  "}.lst-kix_pmrqfhveh3bt-7>li:before{content:"\0025cb  "}.lst-kix_yjwouxw703ue-0>li:before{content:"\0025cf  "}.lst-kix_yjwouxw703ue-8>li:before{content:"\0025a0  "}.lst-kix_pmrqfhveh3bt-8>li:before{content:"\0025a0  "}.lst-kix_yjwouxw703ue-7>li:before{content:"\0025cb  "}.lst-kix_yjwouxw703ue-1>li:before{content:"\0025cb  "}ul.lst-kix_hvza1ngnjmlh-0{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-1{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-2{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-3{list-style-type:none}.lst-kix_yjwouxw703ue-2>li:before{content:"\0025a0  "}ul.lst-kix_hvza1ngnjmlh-4{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-5{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-6{list-style-type:none}ul.lst-kix_hvza1ngnjmlh-7{list-style-type:none}.lst-kix_yjwouxw703ue-3>li:before{content:"\0025cf  "}ul.lst-kix_hvza1ngnjmlh-8{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c12{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c1{font-weight:700;font-family:"Courier New"}.c8{color:#1155cc;text-decoration:underline}.c5{color:#ff0000;font-weight:700}.c0{orphans:2;widows:2}.c2{color:inherit;text-decoration:inherit}.c9{margin-left:36pt;padding-left:0pt}.c3{padding:0;margin:0}.c4{height:11pt}.c7{font-weight:700}.c6{page-break-after:avoid}.c10{height:16pt}.c11{height:14pt}.c13{font-family:"Courier New"}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c12"><p class="c0 c6 title" id="h.8lb1szouoexg"><span>Exercise: Prepare a Cassandra cluster</span></p><h1 class="c0 c6" id="h.mnuysn1tuqk5"><span>Overview</span></h1><p class="c0"><span>In this exercise you will be building a Cassandra cluster in Docker containers on your laptop, go through some node-replacement / rebuild procedures, then shut it all down again.</span></p><h1 class="c0 c6" id="h.qulwwsn0w2uk"><span>Goals</span></h1><ul class="c3 lst-kix_yjwouxw703ue-0 start"><li class="c0 c9"><span>Understand how a cluster is assembled</span></li><li class="c0 c9"><span>Learn how to query the cluster membership state</span></li><li class="c0 c9"><span>Learn how to replace a node</span></li></ul><h1 class="c0 c6" id="h.7d0col3ypy4t"><span>Pre-requisites</span></h1><ul class="c3 lst-kix_pmrqfhveh3bt-0 start"><li class="c0 c9"><span>Docker installed on your machine</span></li><li class="c0 c9"><span>Internet connectivity (for retrieving Docker images)</span></li></ul><h1 class="c0 c6" id="h.61lry14nezh3"><span>Useful Commands</span></h1><h3 class="c0 c6" id="h.2kzs6ng7xqzn"><span>Get the logs from a container</span></h3><p class="c0"><span class="c1">docker logs &lt;container-name/id&gt;</span></p><h3 class="c0 c6" id="h.i16qrhbfuhqf"><span>Run &ldquo;nodetool status&rdquo; in one of your Cassandra containers</span></h3><p class="c0"><span class="c1">docker exec -it &lt;container-name/id&gt; nodetool status</span></p><h3 class="c0 c6" id="h.a3mlyl58230"><span>Run &ldquo;cqlsh&rdquo; in one of your Cassandra containers</span></h3><p class="c0"><span class="c1">docker exec -it &lt;container-name/id&gt; cqlsh</span></p><h3 class="c0 c6" id="h.4zeq3f7tgg5h"><span>Run &ldquo;bash&rdquo; in one of your Cassandra containers</span></h3><p class="c0"><span class="c1">docker exec -it &lt;container-name/id&gt; bash</span></p><hr style="page-break-before:always;display:none;"><h2 class="c0 c6 c10" id="h.rww4g0783pz0"><span></span></h2><h1 class="c0 c6" id="h.oeudgiq6b5cu"><span>Steps</span></h1><h2 class="c0 c6" id="h.61ycotqvjxfq"><span>Build a 3-node cluster</span></h2><p class="c0"><span>This procedure will quickly get a 3-node Cassandra cluster up and running on your personal machine using Docker.</span></p><h3 class="c0 c6" id="h.j53c4kkq4n7o"><span>Create a network for your Cassandra containers</span></h3><p class="c0"><span>The first thing to do is to make a docker network specifically for Cassandra containers (so we can have some control over IP addressing).</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker network create --subnet=172.16.0.0/16 cassandra</span></p><h3 class="c0 c6" id="h.1pxu2alxfx43"><span>Bring up a single-node cluster</span></h3><p class="c0"><span>Now you can bring up your first Cassandra node (172.16.0.11). This container will be called &ldquo;cassandra-1&rdquo;.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.11 --name=cassandra-1 -d cassandra:3.7</span></p><h3 class="c0 c6" id="h.mn2v3e1rmyo6"><span>Check the status of your single-node cluster</span></h3><p class="c0"><span>Your new node will take a couple of seconds to come up. Check the docker logs for the container, and once it looks like it is running you can use nodetool to print out the cluster membership (at this stage you should only see one node). Any node with a &ldquo;UN&rdquo; status is UP and NORMAL.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker exec -it cassandra-1 nodetool status</span></p><h3 class="c0 c6" id="h.hk5ugqujgafz"><span>Introduce a second node</span></h3><p class="c0"><span>If your single-node cluster is running then you should now be able to introduce a second node. Again watch the logs to see how the node goes about joining, and use nodetool to check the cluster membership (note that you can do this on either of the nodes). Notice how we can pass configuration options to Cassandra with the Docker run command (in this case the </span><span class="c8"><a class="c2" href="https://www.google.com/url?q=https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html%23reference_ds_qfg_n1r_1k__seed_provider&amp;sa=D&amp;ust=1476783824451000&amp;usg=AFQjCNHN5n-rUvOMK2W2K4wZWpZUHTEGQw">seed-list</a></span><span>). If you run &ldquo;nodetool status&rdquo; quick enough you may get to see the original node in &ldquo;UN&rdquo; status, and the new one in &ldquo;UJ&rdquo; (UP and JOINING) before they eventually both report &ldquo;UN&rdquo;.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.12 --name=cassandra-2 -d -e CASSANDRA_SEEDS=172.16.0.11 cassandra:3.7</span></p><h3 class="c0 c6" id="h.xembcngdjnne"><span>Introduce a third node</span></h3><p class="c0"><span>If your two-node cluster looks good then you can proceed with adding a third node. Note that this time we&rsquo;re using cassandra-2 as the seed. Since all nodes are peers, any active node can provide seed information.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.13 --name cassandra-3 -d -e CASSANDRA_SEEDS=172.16.0.12 cassandra:3.7</span></p><h2 class="c0 c6 c10" id="h.oc8ymrlikb22"><span></span></h2><hr style="page-break-before:always;display:none;"><h2 class="c0 c6 c10" id="h.ok1p4ogrfevg"><span></span></h2><h2 class="c0 c6" id="h.jfk3ph5mip00"><span>Temporarily take a node out of the cluster</span></h2><p class="c0"><span>This will show you how a Cassandra cluster managed nodes going offline and coming back.</span></p><h3 class="c0 c6" id="h.h02dcof1fyvu"><span>Stop one of the nodes</span></h3><p class="c0"><span>Use the Docker stop command to bring cassandra-2 down, then use the nodetool status command on the remaining nodes to see what they think is going on. Also check the logs on the remaining machines and find any relevant messages.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">docker stop cassandra-2</span></p><h3 class="c0 c6" id="h.ay9mk2kkw6s2"><span>Start the node back up again</span></h3><p class="c0"><span>You can now use the docker start command to bring the cassandra-2 container back to life (its state will have been kept on disk and it should quickly come back up). Use the status command to show that its back, and again check the logs on the other nodes to see what they&rsquo;ve said.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker start cassandra-2</span></p><h3 class="c0 c6" id="h.y0kn0hqvxq0"><span>Consider the implications of a node temporarily going offline</span></h3><p class="c0"><span>Cassandra is designed to gracefully handle nodes temporarily going offline (this is how we would do scheduled maintenance and deal with hardware failures/replacements in the real world). There are however some implications to consider:</span></p><ul class="c3 lst-kix_d7thuibe8wvo-0 start"><li class="c0 c9"><span class="c8"><a class="c2" href="https://www.google.com/url?q=https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_config_consistency_c.html&amp;sa=D&amp;ust=1476783824460000&amp;usg=AFQjCNGCSa84isEmufx_NQlSZI-XgP8ZWQ">What would happen to queries using consistency-level &ldquo;ALL&rdquo; during this time?</a></span></li><li class="c0 c9"><span class="c8"><a class="c2" href="https://www.google.com/url?q=https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_about_hh_c.html?hl%3Dhinted,handoff&amp;sa=D&amp;ust=1476783824461000&amp;usg=AFQjCNHhmuNbLavCQNW5t88Yb3UTSCouXA">What happens to the data that should have replicated to the node that was down during this time?</a></span></li><li class="c0 c9"><span class="c8"><a class="c2" href="https://www.google.com/url?q=https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/opsRepairNodesManualRepair.html?hl%3Drepair&amp;sa=D&amp;ust=1476783824462000&amp;usg=AFQjCNFOAUCl26TJY513zw07NfLpnuHN2w">How can we ensure that data is consistent after an event such as this?</a></span></li><li class="c0 c9"><span class="c8"><a class="c2" href="https://www.google.com/url?q=https://docs.datastax.com/en/cassandra/2.1/cassandra/operations/opsRepairNodesReadRepair.html?hl%3Drepair&amp;sa=D&amp;ust=1476783824462000&amp;usg=AFQjCNGfWlroj7TiDZ_Q3wBt4Y1lwQjFnA">Is there a less-invasive way to fix this data?</a></span></li></ul><hr style="page-break-before:always;display:none;"><p class="c0 c4"><span></span></p><h2 class="c0 c6" id="h.c01nwjoxqg22"><span>Completely replace a node in the cluster</span></h2><p class="c0"><span>We will simulate a complete hardware failure of one of the cluster nodes, and replace it with a new one.</span></p><h3 class="c0 c6" id="h.frxb3kq7rmj7"><span>Stop a node and delete the container</span></h3><p class="c0"><span>Let&rsquo;s pick on cassandra-2 again. The following commands will stop the container and delete it from Docker. Once that is done use nodetool to display the cluster status.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker stop cassandra-2</span></p><p class="c0"><span class="c1">docker rm cassandra-2</span></p><h3 class="c0 c6" id="h.mt93yl25wjpa"><span>Attempt to bring up a replacement</span></h3><p class="c0"><span>The same command we originally used to bring up cassandra-2 can be used again. Don&rsquo;t wait too long for it to join though, because it won&rsquo;t actually succeed! Check the logs for the new cassandra-2 container to find out why.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.12 --name=cassandra-2 -d -e CASSANDRA_SEEDS=172.16.0.11 cassandra:3.7</span></p><h3 class="c0 c6" id="h.ienwpdniblzn"><span>Attempt to bring up an additional node instead</span></h3><p class="c0"><span>If we can&rsquo;t introduce cassandra-2 again with the same IP address as before, surely we could just choose to bring in Cassandra-4 instead? No, this won&rsquo;t work either. Check the logs to find out why.</span></p><p class="c0 c4"><span class="c1"></span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.14 --name cassandra-4 -d -e CASSANDRA_SEEDS=172.16.0.11 cassandra:3.7</span></p><h3 class="c0 c6 c11" id="h.b7mzir6xtbbg"><span></span></h3><hr style="page-break-before:always;display:none;"><h3 class="c0 c6 c11" id="h.9mtm5ae8cxk4"><span></span></h3><h3 class="c0 c6" id="h.nkwgj07vbgt4"><span>Assassinate the dead node from the cluster</span></h3><p class="c0"><span>One way to handle this is to &ldquo;assassinate&rdquo; the dead node from the cluster before attempting to introduce a replacement node. This is potentially </span><span class="c5">danger-zone</span><span>! Run this command on one of the remaining nodes. It will take about 30s, check the logs afterwards to see what happened. Nodetool status should now say that you have 2 nodes in &ldquo;UN&rdquo; state, and nothing else.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker exec -it cassandra-1 nodetool assassinate 172.16.0.12</span></p><h3 class="c0 c6" id="h.4w9247tnk9lm"><span>Cleanup then bring up a replacement</span></h3><p class="c0"><span>The cluster is now ready for a replacement node to join (after cleaning up some junk first).</span></p><p class="c0 c4"><span class="c7"></span></p><p class="c0"><span class="c1">docker rm cassandra-2 cassandra-4</span></p><p class="c0"><span class="c1">docker run --network=cassandra --ip=172.16.0.12 --name=cassandra-2 -d -e CASSANDRA_SEEDS=172.16.0.11 cassandra:3.7</span></p><h3 class="c0 c6" id="h.4rs6gu6zdkex"><span>Run a repair to make sure your data is still consistent</span></h3><p class="c0"><span>If this cluster had any data in it then we will have of course lost some when we killed cassandra-2. This is why for all but the least important datasets we would always maintain multiple replicas (usually 3). This would allow us to &ldquo;repair&rdquo; the data in the cluster by streaming replicas to the new node from its neighbours until we have the desired number of copies again. Try this command on one of the nodes in your cluster.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker exec -it cassandra-1 nodetool repair</span></p><h2 class="c0 c6 c10" id="h.q7gd89yqkiu0"><span></span></h2><hr style="page-break-before:always;display:none;"><h2 class="c0 c6 c10" id="h.fi6pqmb7vpvu"><span></span></h2><h2 class="c0 c6" id="h.agargqyakxu3"><span>Finishing up</span></h2><p class="c0"><span>At this point you should have a healthy 3-node cluster again. This exercise was only concerned with cluster membership and node replacement (no data-verification was performed).</span></p><p class="c0 c4"><span></span></p><p class="c0"><span>In the future you may want to try this exercise again on a cluster with some actual data (once you&rsquo;re happy with creating schemas / replication topologies / inserting).</span></p><p class="c0 c4"><span></span></p><p class="c0"><span>You can now simply stop and remove the Cassandra containers if you don&rsquo;t want them any more, then finally remove the network we created at the beginning.</span></p><p class="c0 c4"><span></span></p><p class="c0"><span class="c1">docker stop cassandra-1 cassandra-2 cassandra-3</span></p><p class="c0"><span class="c1">docker rm cassandra-1 cassandra-2 cassandra-3</span></p><p class="c0"><span class="c1">docker network rm cassandra</span></p></body></html>