<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_olyxhrlalvs1-8>li:before{content:"\0025a0  "}.lst-kix_vpzvmawh0uv8-6>li:before{content:"\0025cf  "}.lst-kix_olyxhrlalvs1-5>li:before{content:"\0025a0  "}.lst-kix_vpzvmawh0uv8-5>li:before{content:"\0025a0  "}.lst-kix_vpzvmawh0uv8-7>li:before{content:"\0025cb  "}.lst-kix_olyxhrlalvs1-4>li:before{content:"\0025cb  "}.lst-kix_olyxhrlalvs1-6>li:before{content:"\0025cf  "}.lst-kix_vpzvmawh0uv8-4>li:before{content:"\0025cb  "}.lst-kix_vpzvmawh0uv8-8>li:before{content:"\0025a0  "}.lst-kix_olyxhrlalvs1-3>li:before{content:"\0025cf  "}.lst-kix_olyxhrlalvs1-7>li:before{content:"\0025cb  "}ul.lst-kix_olyxhrlalvs1-4{list-style-type:none}.lst-kix_oyiaiubt468f-7>li:before{content:"\0025cb  "}.lst-kix_oyiaiubt468f-8>li:before{content:"\0025a0  "}ul.lst-kix_olyxhrlalvs1-5{list-style-type:none}ul.lst-kix_olyxhrlalvs1-2{list-style-type:none}ul.lst-kix_olyxhrlalvs1-3{list-style-type:none}ul.lst-kix_olyxhrlalvs1-0{list-style-type:none}.lst-kix_oyiaiubt468f-5>li:before{content:"\0025a0  "}.lst-kix_oyiaiubt468f-6>li:before{content:"\0025cf  "}ul.lst-kix_olyxhrlalvs1-1{list-style-type:none}ul.lst-kix_olyxhrlalvs1-8{list-style-type:none}ul.lst-kix_olyxhrlalvs1-6{list-style-type:none}ul.lst-kix_olyxhrlalvs1-7{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-7{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-8{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-0{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-1{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-2{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-3{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-4{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-5{list-style-type:none}ul.lst-kix_vpzvmawh0uv8-6{list-style-type:none}.lst-kix_1s44dbzi25qr-4>li:before{content:"\0025cb  "}.lst-kix_1s44dbzi25qr-3>li:before{content:"\0025cf  "}.lst-kix_1s44dbzi25qr-5>li:before{content:"\0025a0  "}.lst-kix_vpzvmawh0uv8-0>li:before{content:"\0025cf  "}.lst-kix_1s44dbzi25qr-2>li:before{content:"\0025a0  "}.lst-kix_1s44dbzi25qr-6>li:before{content:"\0025cf  "}.lst-kix_vpzvmawh0uv8-2>li:before{content:"\0025a0  "}.lst-kix_vpzvmawh0uv8-1>li:before{content:"\0025cb  "}.lst-kix_vpzvmawh0uv8-3>li:before{content:"\0025cf  "}.lst-kix_1s44dbzi25qr-8>li:before{content:"\0025a0  "}.lst-kix_1s44dbzi25qr-7>li:before{content:"\0025cb  "}.lst-kix_ykxyyyuswvww-1>li:before{content:"\0025cb  "}.lst-kix_ykxyyyuswvww-2>li:before{content:"\0025a0  "}.lst-kix_ykxyyyuswvww-3>li:before{content:"\0025cf  "}.lst-kix_ykxyyyuswvww-5>li:before{content:"\0025a0  "}.lst-kix_ykxyyyuswvww-6>li:before{content:"\0025cf  "}ul.lst-kix_ykxyyyuswvww-0{list-style-type:none}.lst-kix_ykxyyyuswvww-4>li:before{content:"\0025cb  "}ul.lst-kix_ykxyyyuswvww-3{list-style-type:none}ul.lst-kix_ykxyyyuswvww-4{list-style-type:none}ul.lst-kix_ykxyyyuswvww-1{list-style-type:none}ul.lst-kix_ykxyyyuswvww-2{list-style-type:none}ul.lst-kix_ykxyyyuswvww-7{list-style-type:none}.lst-kix_ykxyyyuswvww-7>li:before{content:"\0025cb  "}ul.lst-kix_ykxyyyuswvww-8{list-style-type:none}ul.lst-kix_ykxyyyuswvww-5{list-style-type:none}ul.lst-kix_ykxyyyuswvww-6{list-style-type:none}.lst-kix_1s44dbzi25qr-0>li:before{content:"\0025cf  "}.lst-kix_1s44dbzi25qr-1>li:before{content:"\0025cb  "}.lst-kix_ykxyyyuswvww-8>li:before{content:"\0025a0  "}ul.lst-kix_1s44dbzi25qr-6{list-style-type:none}ul.lst-kix_1s44dbzi25qr-7{list-style-type:none}ul.lst-kix_1s44dbzi25qr-8{list-style-type:none}ul.lst-kix_1s44dbzi25qr-2{list-style-type:none}ul.lst-kix_1s44dbzi25qr-3{list-style-type:none}ul.lst-kix_1s44dbzi25qr-4{list-style-type:none}ul.lst-kix_1s44dbzi25qr-5{list-style-type:none}ul.lst-kix_1s44dbzi25qr-0{list-style-type:none}ul.lst-kix_1s44dbzi25qr-1{list-style-type:none}ul.lst-kix_agl4odx2t6x5-6{list-style-type:none}.lst-kix_agl4odx2t6x5-8>li:before{content:"\0025a0  "}ul.lst-kix_agl4odx2t6x5-5{list-style-type:none}.lst-kix_agl4odx2t6x5-7>li:before{content:"\0025cb  "}ul.lst-kix_agl4odx2t6x5-8{list-style-type:none}ul.lst-kix_agl4odx2t6x5-7{list-style-type:none}ul.lst-kix_oyiaiubt468f-2{list-style-type:none}ul.lst-kix_oyiaiubt468f-1{list-style-type:none}.lst-kix_agl4odx2t6x5-3>li:before{content:"\0025cf  "}ul.lst-kix_oyiaiubt468f-4{list-style-type:none}ul.lst-kix_oyiaiubt468f-3{list-style-type:none}.lst-kix_agl4odx2t6x5-2>li:before{content:"\0025a0  "}ul.lst-kix_oyiaiubt468f-6{list-style-type:none}ul.lst-kix_oyiaiubt468f-5{list-style-type:none}ul.lst-kix_oyiaiubt468f-8{list-style-type:none}ul.lst-kix_oyiaiubt468f-7{list-style-type:none}.lst-kix_oyiaiubt468f-0>li:before{content:"\0025cf  "}.lst-kix_oyiaiubt468f-1>li:before{content:"\0025cb  "}.lst-kix_oyiaiubt468f-2>li:before{content:"\0025a0  "}ul.lst-kix_oyiaiubt468f-0{list-style-type:none}.lst-kix_agl4odx2t6x5-4>li:before{content:"\0025cb  "}.lst-kix_oyiaiubt468f-3>li:before{content:"\0025cf  "}.lst-kix_oyiaiubt468f-4>li:before{content:"\0025cb  "}.lst-kix_agl4odx2t6x5-5>li:before{content:"\0025a0  "}.lst-kix_agl4odx2t6x5-6>li:before{content:"\0025cf  "}.lst-kix_olyxhrlalvs1-1>li:before{content:"\0025cb  "}.lst-kix_olyxhrlalvs1-0>li:before{content:"\0025cf  "}.lst-kix_olyxhrlalvs1-2>li:before{content:"\0025a0  "}.lst-kix_agl4odx2t6x5-0>li:before{content:"\0025cf  "}.lst-kix_agl4odx2t6x5-1>li:before{content:"\0025cb  "}ul.lst-kix_agl4odx2t6x5-0{list-style-type:none}ul.lst-kix_agl4odx2t6x5-2{list-style-type:none}ul.lst-kix_agl4odx2t6x5-1{list-style-type:none}.lst-kix_ykxyyyuswvww-0>li:before{content:"\0025cf  "}ul.lst-kix_agl4odx2t6x5-4{list-style-type:none}ul.lst-kix_agl4odx2t6x5-3{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c3{margin-left:36pt;orphans:2;widows:2;padding-left:0pt}.c0{page-break-after:avoid;orphans:2;widows:2}.c11{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c1{orphans:2;widows:2;height:11pt}.c2{font-weight:700;font-family:"Courier New"}.c4{orphans:2;widows:2}.c7{padding:0;margin:0}.c5{font-weight:700}.c6{height:14pt}.c12{height:16pt}.c8{height:10pt}.c9{font-size:10pt}.c10{font-size:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:0pt;color:#000000;font-weight:700;font-size:10pt;padding-bottom:0pt;font-family:"Courier New";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><p class="c0 title" id="h.8lb1szouoexg"><span>Exercise: Replication &amp; consistency</span></p><h1 class="c0" id="h.mnuysn1tuqk5"><span>Overview</span></h1><p class="c4"><span>In this exercise you will explore how data is distributed in a multi-node cluster, how data is replicated, and how this works hand-in-hand with consistency levels.</span></p><h1 class="c0" id="h.qulwwsn0w2uk"><span>Goals</span></h1><ul class="c7 lst-kix_olyxhrlalvs1-0 start"><li class="c3"><span>Understand how a cluster is assembled</span></li><li class="c3"><span>Learn how to query the cluster membership state</span></li><li class="c3"><span>Learn how to replace a node</span></li></ul><h1 class="c0" id="h.7d0col3ypy4t"><span>Pre-requisites</span></h1><ul class="c7 lst-kix_oyiaiubt468f-0 start"><li class="c3"><span>A working multi-node Cassandra cluster as prepared in the clustering exercise (assuming that the containers are called &ldquo;</span><span class="c5">cassandra-1</span><span>&rdquo;, &ldquo;</span><span class="c5">cassandra-2</span><span>&rdquo;, &ldquo;</span><span class="c5">cassandra-3</span><span>&rdquo;).</span></li></ul><h1 class="c0" id="h.61lry14nezh3"><span>Useful Commands</span></h1><h3 class="c0" id="h.i16qrhbfuhqf"><span>Find out which node(s) are responsible for a given key</span></h3><p class="c4"><span class="c2">docker exec -it &lt;container-name/id&gt; nodetool getendpoints</span></p><h3 class="c0" id="h.a3mlyl58230"><span>Find the token for a partition in CQLSH</span></h3><p class="c4"><span class="c2">SELECT TOKEN(user_name) FROM users WHERE user_name=&#39;some-user&#39;;</span></p><h3 class="c0" id="h.8ckff1wepnm"><span>Find the data-distribution for a keyspace</span></h3><p class="c4"><span class="c2">docker exec -it &lt;container-name/id&gt; nodetool status &lt;keyspace&gt;</span></p><hr style="page-break-before:always;display:none;"><p class="c1"><span class="c2"></span></p><h1 class="c0" id="h.iavve58ywnmb"><span>Steps</span></h1><h2 class="c0" id="h.v0q81r6k6y2x"><span>Replication-factor 1</span></h2><p class="c4"><span>The first section works with replication-factor 1, where each partition will randomly be assigned to one of your three nodes. We will run a few experiments to track down our data, and cause a small outage.</span></p><h3 class="c0" id="h.x3xp5f1f95ke"><span>Create a schema &amp; test data</span></h3><h6 class="c0" id="h.tenwx3duf7ez"><span>CREATE KEYSPACE rf_one WITH replication = {&#39;class&#39;: &#39;SimpleStrategy&#39;, &#39;replication_factor&#39;: &#39;1&#39;};</span></h6><h6 class="c0" id="h.tenwx3duf7ez-1"><span>CREATE TABLE rf_one.cruft (</span></h6><h6 class="c0" id="h.tenwx3duf7ez-2"><span>&nbsp; kruftkey varchar,</span></h6><h6 class="c0" id="h.tenwx3duf7ez-3"><span>&nbsp; description varchar,</span></h6><h6 class="c0" id="h.tenwx3duf7ez-4"><span>&nbsp; crufty boolean,</span></h6><h6 class="c0" id="h.tenwx3duf7ez-5"><span>&nbsp; PRIMARY KEY (kruftkey)</span></h6><h6 class="c0" id="h.tenwx3duf7ez-6"><span>);</span></h6><h6 class="c0" id="h.tenwx3duf7ez-7"><span>INSERT INTO rf_one.cruft (kruftkey, description, crufty) VALUES (&#39;testing&#39;, &#39;test key&#39;, true);</span></h6><h3 class="c0" id="h.geq5rfe5hwc2"><span>Find the token</span></h3><p class="c4"><span>Until I can find the algorithm to calculate a Murmur3 token for a given key, we just have to trust CQLSH.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.cdizjkm2g3oa"><span>SELECT TOKEN(kruftkey) FROM rf_one.cruft WHERE kruftkey=&#39;testing&#39;;</span></h6><h3 class="c0" id="h.kia87dg6ggba"><span>Find which node owns that token (the hard way)</span></h3><p class="c4"><span>Use the nodetool command to print a list of all the token-ranges for your cluster, and find where your token fits in. The number in the &ldquo;token&rdquo; column are the upper-boundaries of the token ranges.</span></p><h6 class="c0 c8" id="h.21fjs338hxge"><span></span></h6><h6 class="c0" id="h.cz7xfaw676b3"><span>nodetool ring</span></h6><h3 class="c0" id="h.1e4zm4fgthwj"><span>Find which node owns that token (the easy way)</span></h3><p class="c4"><span>Nodetool has a command which allows you to find which node(s) own your data (by providing the keyspace, table and key).</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.va0jeybcctty"><span>nodetool getendpoints rf_one cruft testing</span></h6><hr style="page-break-before:always;display:none;"><h3 class="c0 c6" id="h.ou1d5l3jaklf"><span></span></h3><h3 class="c0" id="h.753af6597gsb"><span>Inspect the sstable</span></h3><p class="c4"><span>Now you should be able to run bash in the appropriate cassandra container, find the sstable directory for your table, use &ldquo;apt-get install binutils&rdquo; to install the &ldquo;strings&rdquo; binary, flush the table to disk, and use &ldquo;strings&rdquo; to print some contents of the sstable.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.h58l5g7zdocn"><span>apt-get update &amp;&amp; apt-get install binutils</span></h6><h6 class="c0" id="h.oj12nzpex3u0"><span>nodetool flush rf_one</span></h6><h6 class="c0" id="h.eeai9qrdsdka"><span>sstableutil rf_one cruft</span></h6><h6 class="c0" id="h.eeai9qrdsdka-8"><span>strings /var/lib/cassandra/data/rf_one/cruft-xxx/mb-1-big-Data.db</span></h6><h3 class="c0" id="h.7n6u2bbagoxk"><span>Insert 9 more rows</span></h3><p class="c4"><span>Now use CQLSH to insert 9 more rows (hopefully that is enough rolls of the dice to ensure that you end up with some data on each of your 3 nodes).</span></p><h3 class="c0" id="h.bbaq4n4ncp9a"><span>Stop one of your cassandra nodes and try to query your data</span></h3><p class="c4"><span>Use the docker stop command to temporarily stop one of your node, then use cqlsh to query each of your rows in turn. Notice that you can still query the ones which are hosted on nodes that are still up, and the the outage we have caused isn&rsquo;t a complete system outage - merely the data hosted on the missing node.</span></p><p class="c1"><span></span></p><p class="c4"><span>Make sure you start the node again before the next section.</span></p><h2 class="c0 c12" id="h.93ekputw2jmd"><span></span></h2><hr style="page-break-before:always;display:none;"><h2 class="c0 c12" id="h.7kirob6j0kzw"><span></span></h2><h2 class="c0" id="h.dav4m4u37zha"><span>Replication-factor 2</span></h2><p class="c4"><span>In this section we will experiment with storing 2 replicas instead of one, and see what kind of extra availability this can deliver.</span></p><h3 class="c0" id="h.tgqg2gedwgin"><span>Create a schema &amp; test data</span></h3><h6 class="c0" id="h.bmvsqvicfhr6"><span>CREATE KEYSPACE rf_many WITH replication = {&#39;class&#39;: &#39;SimpleStrategy&#39;, &#39;replication_factor&#39;: &#39;2&#39;};</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-9"><span>CREATE TABLE rf_many.cruft (</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-10"><span>&nbsp; kruftkey varchar,</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-11"><span>&nbsp; description varchar,</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-12"><span>&nbsp; crufty boolean,</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-13"><span>&nbsp; PRIMARY KEY (kruftkey)</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-14"><span>);</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-15"><span>INSERT INTO rf_many.cruft (kruftkey, description, crufty) VALUES (&#39;testing&#39;, &#39;test key&#39;, true);</span></h6><h6 class="c0" id="h.bmvsqvicfhr6-16"><span>INSERT INTO rf_many.cruft (kruftkey, description, crufty) VALUES (&#39;exercise&#39;, &#39;another test key&#39;, true);</span></h6><h3 class="c0" id="h.rglon9y2iu1z"><span>Find which nodes own that token</span></h3><p class="c4"><span>Nodetool has a command which allows you to find which node(s) own your data (by providing the keyspace, table and key).</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.cyptt2lbwddo"><span>nodetool getendpoints rf_many cruft testing</span></h6><h3 class="c0" id="h.8r97o5bfm8p7"><span>Insert 9 more rows</span></h3><p class="c4"><span>Now use CQLSH to insert 9 more rows (hopefully that is enough rolls of the dice to ensure that you end up with some data on each of your 3 nodes).</span></p><h3 class="c0" id="h.6qtnzt4jkvni"><span>Shut down one node, try to read your data</span></h3><p class="c4"><span>You should be able to read all of your data in the new keyspace, even with one node out of action. This is because the default consistency-level is ONE.</span></p><h3 class="c0" id="h.awazek7m5hhu"><span>Shut down another node, try to read your data</span></h3><p class="c4"><span>You should still be able to read some of your data, but there is a percentage which is offline now.</span></p><h3 class="c0" id="h.o6otm7ihfh9y"><span>Discuss the options for availability and consistency with RF=2</span></h3><h3 class="c0" id="h.o97ti216mv10"><span class="c10">There are real reasons why 2 is not a good number.</span></h3><hr style="page-break-before:always;display:none;"><h3 class="c0 c6" id="h.khuel5bt9tqi"><span></span></h3><h2 class="c0" id="h.kawr1hq72qx9"><span>Replication-factor 3</span></h2><p class="c4"><span>In this section we will take our first steps with both high-availability and strong-consistency.</span></p><h3 class="c0" id="h.k824ixsdx6u"><span>Start all of your cassandra nodes</span></h3><p class="c4"><span>We need all 3 nodes up and running again.</span></p><h3 class="c0" id="h.35s384ej9w6q"><span>Alter the previous keyspace to have one more replica</span></h3><h6 class="c0" id="h.cj0jsbyq2yon"><span>ALTER KEYSPACE rf_many WITH replication = {&#39;class&#39;: &#39;SimpleStrategy&#39;, &#39;replication_factor&#39;: &#39;3&#39;};</span></h6><h3 class="c0" id="h.8mxqnlo332zb"><span>Repair the keyspace to ensure that we have 3 replicas</span></h3><h6 class="c0" id="h.yv1croo85v5p"><span>nodetool repair rf_many</span></h6><h3 class="c0" id="h.7r3tn59yp3or"><span>Shut down 2 nodes and prove that you can still query all of the data</span></h3><p class="c4"><span>Now that we have data everywhere we can shut down 2 nodes and still run queries at CL=1.</span></p><h3 class="c0" id="h.ywoxsrl1hej6"><span>Start the nodes again, and change consistency</span></h3><p class="c4"><span>Now it is time to try some stronger consistency-levels. Try &ldquo;QUORUM&rdquo; and &ldquo;ALL&rdquo; with tracing enabled and compare the performance to &ldquo;ONE&rdquo;.</span></p><p class="c1"><span class="c5"></span></p><h6 class="c0" id="h.ddlzo9o1hgr"><span class="c9">CONSISTENCY</span><span>&nbsp;QUORUM</span></h6><h6 class="c0" id="h.ddlzo9o1hgr-17"><span class="c9">CONSISTENCY</span><span>&nbsp;ALL</span></h6><h6 class="c0" id="h.ddlzo9o1hgr-18"><span class="c9">CONSISTENCY</span><span>&nbsp;ONE</span></h6><h3 class="c0" id="h.79elahycykab"><span>Perform outage-testing with QUORUM queries</span></h3><p class="c4"><span>&ldquo;QUORUM&rdquo; queries are really the sweet spot with Cassandra. A combination of QUORUM writes and QUORUM reads allows no room for inconsistency, yet can keep working if a node is down. WIN!</span></p><h2 class="c0 c12" id="h.in6a8jk13uzm"><span></span></h2><hr style="page-break-before:always;display:none;"><h2 class="c0 c12" id="h.k09b71hvptn4"><span></span></h2><h2 class="c0" id="h.hnptqe69uatp"><span>Reconciliation</span></h2><p class="c4"><span>We will now try to cause some data-inconsistency, and see how Cassandra copes.</span></p><h3 class="c0" id="h.e7yt84ayasj8"><span>Delete some SSTables</span></h3><p class="c4"><span>We will manually delete the sstables for the rf_many.cruft table we created earlier, then restart cassandra to make it come up with no data for that table.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.qm9q2gpnnyoz"><span>docker exec -it cassandra-3 bash</span></h6><h6 class="c0 c8" id="h.qm9q2gpnnyoz-19"><span></span></h6><h6 class="c0" id="h.qm9q2gpnnyoz-20"><span>rm /var/lib/cassandra/data/rf_many/cruft-*/*</span></h6><h6 class="c0 c8" id="h.qm9q2gpnnyoz-21"><span></span></h6><h6 class="c0" id="h.qm9q2gpnnyoz-22"><span>exit</span></h6><h6 class="c0 c8" id="h.qm9q2gpnnyoz-23"><span></span></h6><h6 class="c0" id="h.qm9q2gpnnyoz-24"><span>docker restart cassandra-3</span></h6><h3 class="c0" id="h.97ow473mexbk"><span>Query the data with CL=one</span></h3><p class="c4"><span>Once cassandra has loaded, use cqlsh to query the row we inserted before. You will probably not get a response!</span></p><p class="c1"><span class="c2"></span></p><h6 class="c0" id="h.syemy689kpkt"><span>docker exec -it cassandra-3 cqlsh -C</span></h6><h6 class="c0 c8" id="h.syemy689kpkt-25"><span></span></h6><h6 class="c0" id="h.syemy689kpkt-26"><span>CONSISTENCY ONE</span></h6><h6 class="c0" id="h.syemy689kpkt-27"><span>SELECT * FROM rf_many.cruft WHERE kruftkey = &#39;testing&#39;;</span></h6><h3 class="c0" id="h.ud42mncyyjq3"><span>Query the data with CL=quorum</span></h3><p class="c4"><span>But all is not lost&hellip; we can use a higher consistency-level.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.thmhtahvhosw"><span>CONSISTENCY QUORUM</span></h6><h6 class="c0" id="h.thmhtahvhosw-28"><span>SELECT * FROM rf_many.cruft WHERE kruftkey = &#39;testing&#39;;</span></h6><h3 class="c0" id="h.xaqivd8oir4z"><span>Try again with CL=one</span></h3><p class="c4"><span>And now Cassandra&rsquo;s background consistency-reconciliation will have permanently repaired this record on cassandra-3.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.7r369cm9u46q"><span>CONSISTENCY ONE</span></h6><h6 class="c0" id="h.7r369cm9u46q-29"><span>SELECT * FROM rf_many.cruft WHERE kruftkey = &#39;testing&#39;;</span></h6><h3 class="c0" id="h.jea79yln3nz7"><span>Repair the rest of the data</span></h3><p class="c4"><span>Now use the nodetool repair command to repair the table before checking the other rows.</span></p><p class="c1"><span></span></p><h6 class="c0" id="h.57xcv71mfzbk"><span>nodetool repair rf_many cruft -full</span></h6><p class="c1"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c1"><span></span></p><h1 class="c0" id="h.ic34yl74moq9"><span>Finishing up</span></h1><p class="c4"><span>Hopefully by this point you have a better understanding of how data is replicated and distributed around a Cassandra cluster.</span></p><p class="c1"><span></span></p><p class="c4"><span>The important points to take away are:</span></p><ul class="c7 lst-kix_vpzvmawh0uv8-0 start"><li class="c3"><span>An understanding of how data gets distributed according to the replication factor</span></li><li class="c3"><span>Appreciating that multiple replicas can be queried using different consistency levels</span></li><li class="c3"><span>A memory of having seen Cassandra automatically repair inconsistent data</span></li><li class="c3"><span>The knowledge that you can always fix inconsistencies using a repair</span></li></ul></body></html>